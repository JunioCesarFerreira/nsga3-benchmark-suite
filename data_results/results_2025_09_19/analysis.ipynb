{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e011853",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 32.2 GiB for an array with shape (65780, 65780) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRean√°lise conclu√≠da. Resultados salvos em \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# --- EXECU√á√ÉO ---\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[43mreanalyze_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRADII\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 68\u001b[0m, in \u001b[0;36mreanalyze_experiment\u001b[1;34m(results_dir, output_dir, radii)\u001b[0m\n\u001b[0;32m     65\u001b[0m     all_metrics\u001b[38;5;241m.\u001b[39msetdefault(impl, {})\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m radii:\n\u001b[1;32m---> 68\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mreanalyze_coverage_for_radius\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpareto_front\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         all_metrics[impl]\u001b[38;5;241m.\u001b[39msetdefault(r, [])\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# agrega√ß√£o\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mreanalyze_coverage_for_radius\u001b[1;34m(pareto_front, ref_pts, radius)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreanalyze_coverage_for_radius\u001b[39m(pareto_front, ref_pts, radius):\n\u001b[0;32m     27\u001b[0m     ptin, ptout \u001b[38;5;241m=\u001b[39m count_points_per_niche_dtlz2(pareto_front, ref_pts, radius)\n\u001b[1;32m---> 28\u001b[0m     niche_metrics \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_niche_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mradius\u001b[39m\u001b[38;5;124m\"\u001b[39m: radius,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: ptin\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints_out\u001b[39m\u001b[38;5;124m\"\u001b[39m: ptout,\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mniche_metrics\n\u001b[0;32m     34\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\jcf_s\\source\\git\\nsga3-benchmark-suite\\analysis\\coverege_per_niche.py:62\u001b[0m, in \u001b[0;36manalyze_niche_distribution\u001b[1;34m(counts, n_fora)\u001b[0m\n\u001b[0;32m     60\u001b[0m ent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum([pi \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(pi) \u001b[38;5;28;01mfor\u001b[39;00m pi \u001b[38;5;129;01min\u001b[39;00m p \u001b[38;5;28;01mif\u001b[39;00m pi \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     61\u001b[0m ent_norm \u001b[38;5;241m=\u001b[39m ent \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(K) \u001b[38;5;28;01mif\u001b[39;00m K \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 62\u001b[0m gini \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m K \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     63\u001b[0m expected \u001b[38;5;241m=\u001b[39m counts\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m K\n\u001b[0;32m     64\u001b[0m chi2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((counts \u001b[38;5;241m-\u001b[39m expected) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m expected) \u001b[38;5;28;01mif\u001b[39;00m expected \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 32.2 GiB for an array with shape (65780, 65780) and data type float64"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\")) \n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "from analysis.coverege_per_niche import count_points_per_niche_dtlz2, analyze_niche_distribution\n",
    "from utils.generate_points import generate_reference_points\n",
    "\n",
    "# --- CONFIG ---\n",
    "EXPERIMENT = \"e8_1\"\n",
    "RESULTS_DIR = Path(EXPERIMENT)   # deve rodar dentro de \"results/\"\n",
    "OUTPUT_DIR = Path(\"reanalyzed\") / EXPERIMENT\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Raios que voc√™ quer testar\n",
    "RADII = [0.15, 0.20, 0.25, 0.3, 0.35]\n",
    "\n",
    "# --- FUN√á√ïES ---\n",
    "\n",
    "def reanalyze_coverage_for_radius(pareto_front, ref_pts, radius):\n",
    "    ptin, ptout = count_points_per_niche_dtlz2(pareto_front, ref_pts, radius)\n",
    "    niche_metrics = analyze_niche_distribution(ptin, ptout)\n",
    "    return {\n",
    "        \"radius\": radius,\n",
    "        \"points_in\": ptin.tolist(),\n",
    "        \"points_out\": ptout,\n",
    "        **niche_metrics\n",
    "    }\n",
    "\n",
    "def reanalyze_experiment(results_dir: Path, output_dir: Path, radii: list[float]):\n",
    "    all_metrics = {}\n",
    "\n",
    "    for file in results_dir.glob(\"run_*_*.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        impl = data[\"implementation\"]\n",
    "        pareto_front = np.array(data[\"pareto_front\"], dtype=float)\n",
    "\n",
    "        # par√¢metros originais do experimento\n",
    "        num_obj = data.get(\"num_obj\") or None\n",
    "        divisions = data.get(\"divisions\") or None\n",
    "\n",
    "        if num_obj is None or divisions is None:\n",
    "            # tenta pegar do summary se n√£o estiver em cada arquivo\n",
    "            # fallback: usar valores globais do experimento\n",
    "            summary_file = results_dir / \"summary.json\"\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file, \"r\") as sf:\n",
    "                    summary = json.load(sf)\n",
    "                params = summary.get(\"parameters\", {})\n",
    "                num_obj = params.get(\"num_obj\", 2)\n",
    "                divisions = params.get(\"divisions\", 10)\n",
    "            else:\n",
    "                num_obj, divisions = 2, 10  # defaults\n",
    "\n",
    "        ref_pts = generate_reference_points(num_obj, divisions)\n",
    "\n",
    "        all_metrics.setdefault(impl, {})\n",
    "\n",
    "        for r in radii:\n",
    "            res = reanalyze_coverage_for_radius(pareto_front, ref_pts, r)\n",
    "            all_metrics[impl].setdefault(r, []).append(res)\n",
    "\n",
    "    # agrega√ß√£o\n",
    "    aggregated = {}\n",
    "    for impl, rad_dict in all_metrics.items():\n",
    "        aggregated[impl] = {}\n",
    "        for r, metrics_list in rad_dict.items():\n",
    "            keys = [k for k in metrics_list[0].keys() if k not in (\"radius\", \"points_in\", \"points_out\")]\n",
    "            mean_metrics = {}\n",
    "            for k in keys:\n",
    "                vals = [m[k] for m in metrics_list]\n",
    "                mean_metrics[k] = float(np.mean(vals))\n",
    "            aggregated[impl][r] = mean_metrics\n",
    "\n",
    "    # salvar em JSON\n",
    "    with open(output_dir / \"reanalyzed_coverage.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"radii\": radii,\n",
    "            \"aggregated\": aggregated\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # salvar em CSV\n",
    "    csv_file = output_dir / \"reanalyzed_coverage.csv\"\n",
    "    with open(csv_file, \"w\", newline=\"\") as fcsv:\n",
    "        fieldnames = [\"implementation\", \"radius\"] + list(next(iter(next(iter(aggregated.values())).values())).keys())\n",
    "        writer = csv.DictWriter(fcsv, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for impl, rad_dict in aggregated.items():\n",
    "            for r, metric_dict in rad_dict.items():\n",
    "                row = {\"implementation\": impl, \"radius\": r}\n",
    "                row.update(metric_dict)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Rean√°lise conclu√≠da. Resultados salvos em {output_dir}\")\n",
    "\n",
    "# --- EXECU√á√ÉO ---\n",
    "reanalyze_experiment(RESULTS_DIR, OUTPUT_DIR, RADII)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec51fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reanalisando e8_1 com refer√™ncia [5, 5, 5, 5, 5, 5]...\n",
      "üîÑ Reanalisando e8_2 com refer√™ncia [5, 5, 5, 5, 5, 5]...\n",
      "‚úÖ Rean√°lise conclu√≠da. Resultados salvos em reanalyzed_hv\\reanalyzed_hv_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\")) \n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "from analysis.indicators import hypervolume\n",
    "\n",
    "# --- CONFIG ---\n",
    "EXPERIMENTS = [\"e8_1\", \"e8_2\"]\n",
    "\n",
    "# pontos de refer√™ncia (um por experimento 1..7, na mesma ordem)\n",
    "REFERENCE_POINTS = [\n",
    "    [5]*6, # experiment6\n",
    "    [5]*6  # experiment7\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = Path(\"reanalyzed_hv\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def reanalyze_hv_experiment(results_dir: Path, ref_point: list[float]):\n",
    "    \"\"\"Recalcula HV m√©dio para cada implementa√ß√£o em um experimento\"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    for file in results_dir.glob(\"run_*_*.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        impl = data[\"implementation\"]\n",
    "        pareto_front = np.array(data[\"pareto_front\"], dtype=float)\n",
    "\n",
    "        hv = hypervolume(pareto_front, ref_point)\n",
    "\n",
    "        stats.setdefault(impl, []).append(hv)\n",
    "\n",
    "    # agrega√ß√£o: m√©dia por implementa√ß√£o\n",
    "    aggregated = {}\n",
    "    for impl, values in stats.items():\n",
    "        aggregated[impl] = {\n",
    "            \"mean_hv\": float(np.mean(values)),\n",
    "            \"std_hv\": float(np.std(values)),\n",
    "            \"num_runs\": len(values)\n",
    "        }\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def reanalyze_all_experiments(base_dir: Path, experiments: list[str], ref_points: list[list[float]]):\n",
    "    summary = {\n",
    "        \"parameters\": {\n",
    "            \"experiments\": experiments,\n",
    "            \"reference_points\": ref_points\n",
    "        },\n",
    "        \"results\": {}\n",
    "    }\n",
    "\n",
    "    for exp, ref in zip(experiments, ref_points):\n",
    "        results_dir = base_dir / exp\n",
    "        if not results_dir.exists():\n",
    "            print(f\"‚ö† Diret√≥rio {results_dir} n√£o encontrado, ignorando\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üîÑ Reanalisando {exp} com refer√™ncia {ref}...\")\n",
    "        aggregated = reanalyze_hv_experiment(results_dir, ref)\n",
    "        summary[\"results\"][exp] = aggregated\n",
    "\n",
    "    # salvar em JSON\n",
    "    out_file = OUTPUT_DIR / \"reanalyzed_hv_summary.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Rean√°lise conclu√≠da. Resultados salvos em {out_file}\")\n",
    "\n",
    "\n",
    "# --- EXECU√á√ÉO ---\n",
    "reanalyze_all_experiments(Path('.'), EXPERIMENTS, REFERENCE_POINTS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
